This Week I will be Laying the Groudwork for Competitive Strategy
In the Week 2 and 3 my agent learned how to detect emerging consumer trends, these next two weeks my agent will see what competiters, such as tracking product updates, pricing changes, and feature trends from retailers like Amazon and Walmart.

competitive monitoring comes in: the process of tracking rival products, prices, features, campaigns, and reviews to guide smarter decisions.
  - Matched products by attributes, not just SKUs / Automated price intelligence / Tracked trends by category / Delivered measurable impact

Analyze one competing retailer in category of rugs: will closley look at their product listings, prices, styles, and messaging to spot patterns, gaps, and opportunities

If your agent surfaced these five signals automatically next week, which one Wayfair decision would you recommend to act on immediately (merch, marketing, or product)? Explain in one sentence why.: Act on the handloom/woven accent rug trend immediately through merchandising, since adding or featuring these versatile small rugs aligns with current consumer demand and can drive quick sales.

Will be scraping to analyze competitor: 
In Project 2, your agent was an anthropologist, studying behavior across the market.
In Project 3, it’s a detective, zooming in on specific suspects to find clues about strategy.

Scraping = structured visibility- turns messy web pages into clean, comparable product data your agent can analyze.

- Fetch Wayfair Rugs Page (HTTP Request): 
    Purpose: Download the Wayfair listing page so the workflow has the raw content it needs to find product information.
    Action: This node sends a normal web request (a GET) to the Wayfair rugs listing URL. We include a browser-like User-Agent header so the response looks like it’s coming back to a regular shopper’s browser. The node captures whatever Wayfair returns — that could be HTML, or sometimes a JSON blob embedded in the page — and stores it for the next node to inspect.

-Extract Rugs Products (Code)
    Purpose: Find the product records inside the raw response and turn them into tidy, consistent fields that the rest of the workflow can use.
    Action: This code node reads the data that the HTTP Request returned and safely parses it (it can handle raw text, JSON strings, or arrays). It looks specifically for Wayfair’s product list object (commonly browse.browse_grid_objects) and then pulls out the useful fields from each product card: product name, SKU, product URL, price info, image ID (used to build an image URL), ratings and review counts, material/features, size options, shipping flags, and so on. The result is a clean list of product objects where each item has the same set of labeled fields — essentially turning messy web content into a structured product table.

- Filter Valid Products (Filter)
    Purpose: Keep only the meaningful product records so downstream steps don’t waste time on empty or broken data.
    Action: This node checks each item coming from the extractor and removes anything that looks invalid — for example, records without a product title. It’s a simple quality gate: if a product lacks the minimum required data (like a name), it’s dropped here so that later steps only work with useful entries.

-Create Final JSON (Code)
    Purpose: Package all the cleaned product rows into one tidy JSON payload with metadata so other parts of the system can consume the entire scrape as a single object.
    Action: This code node collects the filtered product items and bundles them into one JSON document. It creates a small header called scrape_info (with the source name, category, timestamp, and total count) and places the product list under wayfair_products. Think of it as preparing a finished spreadsheet and adding a cover sheet that explains where it came from and when it was made.

-Debug / Pass-through Logger (Code)
    Purpose: Let you inspect and confirm the full JSON structure before sending it downstream or merging it with other data sources.
    Action: This final code node prints the complete JSON to the workflow logs (so you can visually inspect it in n8n), and then passes the same object forward unchanged. It’s a safe checkpoint for debugging — if anything looks off, you can open the node’s execution output and see the raw, final result.

Wayfair Scraper Flow Image

Which node do you think adds the most value to your agent and why? 
The scraping code node adds the most value because it handles data extraction, pagination, error handling, and site-specific challenges. The URL is just an input, while the logic determines data quality and reliability. 

If you were part of Wayfair’s team, how could this data help drive more informed product or pricing decisions as early as next week?
This data would help identify immediate pricing gaps and competitive pressure by comparing Wayfair products to Amazon in real time, enabling quick price adjustments or targeted promotions on high-demand items.

Wayfair and Amazon Scrapper (Current Flow Image) 

Build the Analyst Chain 
  - Merge1 = merges the amazon scrappers into one stream
    - Purpose: Collects all Amazon-derived items into a single stream.
    -Action: Takes the outputs from the Amazon scraper and appends them together so downstream nodes see a unified list.
    -Why this matters: Amazon data can come from multiple scrapers or discovery paths. Merge1 ensures duplicates and different formats are funneled into one place for consistent processing

  - Merge2= merges Merge1 and Wayfair scrapper: cleaner,easier to debug and scalable if need to add more competitors later on.
    -Purpose: Combine Amazon data with Wayfair data into one consolidated feed.
    -Action: Appends the output of Merge1 (all Amazon items) with the Wayfair scraper output, producing a single combined stream of competitor + Wayfair records.
    -Why this matters: The AI agent needs both competitor and Wayfair records side-by-side to make comparisons. Merge creates that single dataset so the agent can analyze relationships, gaps, and overlaps.

  - Analyze All Data= AI Agent Node: takes everything the scrapers bring in and synthessizes while using Google Gemini as the chat model 
    -Purpose: Act as the analyst that ingests the combined data and produces a structured intelligence output.
    -Action: The agent interprets the data, extracts trends and comparisons, and outputs a single HTML report string.
    -Why this matters: This node is the decision-making core: it applies the business rules, normalization, and reporting templates you defined to turn raw items into insights.
  Google Gemini 
    -Purpose: Generate natural, well-structured text (the written report) from the agent’s instructions and the combined data.
    -Action: The AI Agent node is connected to the Gemini model. Gemini converts the agent’s parsed instructions into the final HTML-formatted content (tables, bullets, summary text) according to the style guide.
    -Why this matters: The agent provides the logic and structure; Gemini supplies the fluent narrative and formatting. Together they produce a readable, shareable report.

  -Clean AI Output= code node that cleans up input: takes messy fraft HTML from the AI and removes junk, fixes tags, and ensures the final document is readable and renderable. 
    -Purpose: Sanitize, normalize, and ensure the AI’s HTML is safe and renderable.
    -Action: Runs JavaScript to remove markdown fences, tidy escaped newlines/tabs, convert basic markdown to HTML where needed, remove duplicate <html>/<body> wrappers, and ensure the output is wrapped with <!DOCTYPE html> if missing.
    -Why this matters: AI-generated HTML can be messy. This node acts like an editor that fixes formatting issues so the output displays correctly and can be saved without manual cleanup.

  - Download HTML File = code node: packages HTML reports like a .zip file, converting it into downloadable brinary data that can be previewed or shared
    -Purpose: Turns your cleaned HTML into a downloadable file.
    -Action: The code takes your HTML, converts it into a special “file-ready” format (base64), and names it file.html so it can be downloaded or shared.
    -Why this matters: This step makes your AI-generated report usable: you can now save it, share it with teammates, or plug it into a dashboard later.
  - See HTML Output here= allows you to see the template 
    -Purpose: Preview the cleaned HTML report directly inside n8n.
    -Action: Renders {{ $json.cleaned_minified_html }} in the node UI so you can quickly inspect the final report without downloading.
    -Why this matters: Fast visual verification: you can see layout, headings, tables, and copy immediately and iterate on the system message or scrapers if something looks off.
